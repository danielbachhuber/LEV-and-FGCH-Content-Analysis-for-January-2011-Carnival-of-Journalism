<h2>Carnival of Journalism: "Universities as hubs of journalistic activity"</h2>

In the first Carnival of Journalism of the new year, <a href="http://carnivalofjournalism.com/2010/10/22/hello-world/">David Cohn asks</a>: How do we increase the role of higher education as hubs of journalistic activity?

First, the why. Educational institutions often have long-standing ties to a local community, both in terms of physical location as well as relationships. In New York City, there are often families with multiple generations who have attended CUNY. Educational institutions are also in a unique position where they have access to continually fresh human capital. These are the strategic advantages.

As to the how, there are dozens of projects we could embark on. For instance, we could team with computer science students build a tool that <a href="http://danielbachhuber.com/2010/05/09/questions-currently-of-interest/#h[WitWit]">maps a community's information needs</a>. Or we could offer low-cost multimedia reporting courses to active commenters in hopes they will take the initiative to cover their own neighborhoods. Or we could reorient the entire institution to be a working newsroom and task hundreds of students as boots-on-the-ground reporters.

Before going off the deep-end, it's critically important to ground big ideas in the reality of today. Educational institutions are cautious publishers because of the possibility of libel. More often than not, their technology departments are tuned for providing IT support, not innovating with content management systems. Once you have this valuable context, it's more straight-forward how you can make legitimate change. We can hack technology and we can hack systems.

How you frame the problem also determines the success of your solution. In my mind, "hubs of journalistic activity" to be providing a space for communities to: access accurate and impartial information, learn about topics affecting their civic well-being, and make <em>enlightened</em> decisions. These topics range from transportation to education to environment to governance. For many institutions of higher education, the local community is where they can have the most significant impact.

<h3>The Locals: East Village and Fort Greene-Clinton Hill</h3>

Let's use data and interviews to paint a picture of university-sponsored hyperlocal journalism as it currently exists in New York City. As examples, look at The Local East Village (LEV) and The Local Fort Greene-Clinton Hill (FGCH) during November 2010. The LEV is an operation run out of NYU, has content contributed to it by roughly 35 students from an elective class called "The Hyperlocal Newsroom," and is edited full-time by <a href="http://journalism.nyu.edu/faculty/richard-jones/">Rich Jones</a>. Kim Davis was the community editor in November. The FGCH website has been a CUNY Graduate School of Journalism project <a href="http://www.journalism.cuny.edu/2010/01/08/cuny-j-school-to-take-over-nytimes-coms-the-local-community-web-site/">since January 2010</a> and is managed by two alumni, <a href="http://www.annaliesegriffin.com/">Annaliese Griffin</a> and <a href="http://www.indraniclips.com/">Indrani Sen</a>. Two smaller classes participated in creating content for the website.

Both publications held weekly budget meetings last November to discuss story ideas generated from emails from the community, press releases, watching other local news operations, and general brainstorming. When not in the same physical space, communicating the editorial workflow included a combination of email, Google Chat, text and phone. Breaking news generally received a same-day turnaround while other pieces like features and multimedia had a longer production cycle.

Just by the numbers, the LEV, in its 3rd month of operation, published 100 posts from 48 authors totaling 46,289 words. Rich Jones feels this is "roughly 70 to 80 percent" of the story ideas they generated. 33 posts were from 19 community contributors and their posts were 369 words on average.

While trying to cover everything, Rich explains the "challenge for students is working around their class schedules for story assignments and filing stories" and the "challenge regarding the community is helping them become accustomed to professional-level standards that some – through not fault of their own – are largely unfamiliar with." He also says "feature stories are most likely to be picked up by contributors. The most difficult stories appear to be hard news pieces (fires, crime, etc.) which involve multiple sources and contacting the authorities such as police and fire officials."

The most discussed LEV stories based on comments were "<a href="http://eastvillage.thelocal.nytimes.com/2010/11/10/examining-m15-bus-line-changes/">Examining M15 Bus Line Changes</a>", and <a href="http://eastvillage.thelocal.nytimes.com/2010/11/02/locals-john-penley/">a profile of John Penley</a>, both which received 5 comments. On average, posts had 0.8 comments a piece. The Local East Village had a grand total of 80 unique commenters.

To compare, in November 2010, FGCH published 105 posts from 46 authors totaling 46,945 words. Annaliese Griffin thinks this to be "maybe 75 percent" of the story ideas they generated, but explains it "always feels like there’s so much more we’re not covering." 36 posts were from 23 community contributors and their posts were, surprisingly, 610 words on average.

When asked about the successes and challenges of working with students and community contributors, Annaliese explains:

<blockquote>Scheduling is the big problem with students. They have to balance Local assignments with stories for three other classes. You can’t just say, "This event is happening today, you have to cover it." It’s always a negotiation, which ties your hands as an editor. The successes have been many, though. Seeing students gain story sense and understand not only what needs to be covered, but how best to cover a community has been great. Also, we worked on several crowdsourcing projects that were fun and innovative. <a href="http://fort-greene.thelocal.nytimes.com/2010/12/16/the-day-favorite-stories-from-the-semester/">Here’s a post outlining my favorite student work from last semester</a>. With community contributors we’ve found that straight news, especially crime and anything dealing with city agencies is a challenge. Reporting is hard work that takes skill that needs to be developed over years, not afternoons. We’ve gotten great features and opinion pieces from community contributors, and our schools coverage is about half reported by students, half community contributors and concerned parents. It’s a nice mix.</blockquote>

She adds "schools, arts, and local going-ons" are good fodder for community contributions, and that they want The Local to be the communication platform for micro concerns like "cracked sidewalks, trash pick up, noise complaints, [and] great art programs in schools."

The most discussed stories based on comments were "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/12/imho-dont-leave-district-13-stay-and-help/">IMHO: Don't Leave District 13, Stay and Help</a>" (71 comments), "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/29/opinion-lets-work-together-parents/">Opinion: Let's Work Together, Parents</a>" (45 comments), and "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/05/moes-bar-to-close-in-february/">Moe's Bar to Close in February</a>" (32 comments). On average, posts had 4.5 comments a piece. The Fort Greene-Clinton Hill website had a grand total of 303 unique commenters.

On the technology side, both sites are on the NY Times' blog network and running WordPress 2.9.2. Images and photo galleries are managed by Flickr.

<h3>Grand ideas and continual improvement</h3>

Problems we can solve:
* Finding the right student or community member to cover a story at any given time

We need to make a switch from objective journalism to objective-based journalism. "<a href="http://korrvalues.com/2010/01/30/objectivity-isnt-truthful-its-pathological/">Objectivity isn’t truthful — it’s pathological</a>." Objective-based journalism starts with the understanding that, if you can put the information out there and ensure quality, it's going to be useful to someone. Next, it asks: what are all of the topics our community needs information on, and <a href="http://pressthink.org/2010/10/the-100-percent-solution-for-innovation-in-news/">how do we get to 100% coverage</a> on each?

* Better tools for understanding variances in the community's information priorities.

* Community members can connect their news organization accounts with Foursquare. When spot news breaks, the news organization has working intelligence on who on the ground to cover it.
* Build a profile of the community's knowledge and expertise. This could be as simple as seeing which articles they comment on, and how the community reacts to those comments. If someone is writing A+ comments regularly, and those comments are a few paragraphs long, invite them to write a guest entry.

<h3>Methodology</h3>

For those interested, my <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism">code and data is posted on GitHub</a>. You can get a sense of how the stats collection progressed (and also how my post evolved) from the <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism/commits">commit log</a>.

Once I established the most effective way, collecting the data was actually quite simple. WordPress usually has a URL structure that supports filtering posts by date. Then, for any given URL, you can access an RSS feed for all of the posts on that date by appending "/feed/".

The <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism/blob/master/download_and_parse_ftg_content.php">script I wrote</a> generated day URLs for the entire month (e.g. <a href="http://eastvillage.thelocal.nytimes.com/2010/11/28/">http://eastvillage.thelocal.nytimes.com/2010/11/28/</a>), downloaded the RSS feed, and parsed out the permalink and post ID for each post. <a href="http://magpierss.sourceforge.net/">MagpieRSS</a> is a simple PHP-based RSS parser for the job.

Using the permalinks identified from the RSS feed, I used another tool called <a href="http://simplehtmldom.sourceforge.net/">PHP Simple HTML DOM Parser</a> to download the entire HTML page for the post, and then parse out the data points I needed, including author, author type, body content, post category and tags. Once I had the data, I saved it to the database.

If the site you're scraping gives you full body content in the feed, you can probably skip the second step.

The only downside to accessing the original permalinks by RSS is that WordPress offers 10 items per feed by default, and doesn't support pagination. If there were more than 10 posts published in a day, you'd miss data. Similarly, if there are more than 10 comments, you only get the first 10. Because of this, my data set only has 10 of N comments for several of the FGCH posts.

There are a few things I would like to have tabulated if I had more time. First, I'd go through each post and assign it a category based on the type 