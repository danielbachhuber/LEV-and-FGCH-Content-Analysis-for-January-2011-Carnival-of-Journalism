<h2>Carnival of Journalism: "Universities as hubs of journalistic activity"</h2>

In the first Carnival of Journalism of the new year, <a href="http://carnivalofjournalism.com/2010/10/22/hello-world/">David Cohn asks</a>: How do we increase the role of higher education as hubs of journalistic activity?

The key phrases here are "higher education" and "hubs of journalistic activity." To frame my response, I'd like to define those terms. "Higher education" I'll assume is close to the <a href="http://en.wikipedia.org/wiki/Higher_education">Wikipedia definition</a>, or "a level of education that is provided at academies, universities, colleges, seminaries, institutes of technology, and certain other collegiate-level institutions, such as vocational schools, trade schools, and career colleges, that award academic degrees or professional certifications." Sweet, simple enough.

"Hubs of journalistic activity" is more nebulous, especially when we don't have a <a href="http://danielbachhuber.com/2009/02/26/what-is-journalism/">working definition of journalism</a>. I think it means to provide a context for communities to: access accurate and impartial information, learn about topics affecting their civic well-being, and make <em>enlightened</em> decisions. These topics range from transportation to education to environment to governance. For many institutions of higher education, the local community is where they can have the most significant impact.

<h3>The Locals: East Village and Fort Greene-Clinton Hill</h3>

Let's use data to paint a picture of university-sponsored hyperlocal journalism as it currently exists in New York City. As examples, look at what The Local East Village (LEV) and The Local Fort Greene-Clinton Hill (FGCH) were like in November 2010. The LEV is an operation run out of NYU, has content contributed to it by roughly 35 students from an elective class called "The Hyperlocal Newsroom," and is edited full-time by <a href="http://journalism.nyu.edu/faculty/richard-jones/">Rich Jones</a>. Kim Davis was the community editor in November. The FGCH website has been a CUNY Graduate School of Journalism project <a href="http://www.journalism.cuny.edu/2010/01/08/cuny-j-school-to-take-over-nytimes-coms-the-local-community-web-site/">since January 2010</a> and is managed by two alumni, <a href="http://www.annaliesegriffin.com/">Annaliese Griffin</a> and <a href="http://www.indraniclips.com/">Indrani Sen</a>. Two smaller classes participated in creating content for the website.

Both publications held weekly budget meetings. When not in the same physical space, communicating the editorial workflow included a combination of email, Google Chat, text and phone.

In November, the LEV, just in its 3rd month of operation, published 100 posts from 48 authors totaling 46,289 words. 33 posts were from 19 community contributors and their posts were 369 words on average.

The most discussed stories based on comments were "<a href="http://eastvillage.thelocal.nytimes.com/2010/11/10/examining-m15-bus-line-changes/">Examining M15 Bus Line Changes</a>", a profile of  although no post received more than five comments.

To compare, in November 2010, FGCH published 105 posts from 46 authors totaling 46,945 words. 36 posts were from 23 community contributors and their posts were, surprisingly, 610 words on average.

Most discussed stories based on comments were "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/12/imho-dont-leave-district-13-stay-and-help/">IMHO: Don't Leave District 13, Stay and Help</a>" (71 comments), "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/29/opinion-lets-work-together-parents/">Opinion: Let's Work Together, Parents</a>" (45 comments), and "<a href="http://fort-greene.thelocal.nytimes.com/2010/11/05/moes-bar-to-close-in-february/">Moe's Bar to Close in February</a>" (32 comments).

On the technology side, both sites are on the NY Times' blog network running WordPress 2.9.2. Images and photo galleries are managed by Flickr.

<h3>Big ideas and continual improvement</h3>

We need to make a switch from objective journalism to objective-based journalism. "<a href="http://korrvalues.com/2010/01/30/objectivity-isnt-truthful-its-pathological/">Objectivity isn’t truthful — it’s pathological</a>." Objective-based journalism starts with the understanding that, if you can put the information out there and ensure quality, it's going to be useful to someone. Next, it asks: what are all of the topics our community needs information on, and <a href="http://pressthink.org/2010/10/the-100-percent-solution-for-innovation-in-news/">how do we get to 100% coverage</a> on each?

* Better tools for understanding variances in the community's information priorities.

* Community members can connect their news organization accounts with Foursquare. When spot news breaks, the news organization has working intelligence on who on the ground to cover it.
* Build a profile of the community's knowledge and expertise. This could be as simple as seeing which articles they comment on, and how the community reacts to those comments. If someone is writing A+ comments regularly, and those comments are a few paragraphs long, invite them to write a guest entry.

<h3>Methodology</h3>

For those interested, my <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism">code and data is posted on GitHub</a>. You can get a sense of how the stats collection progressed (and also how my post evolved) from the <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism/commits">commit log</a>.

Once I established the best way to do it, collecting the data was actually quite simple. WordPress usually has a URL structure that supports filtering posts by date. Also, for any given URL, you can access an RSS feed by appending "/feed/".

The <a href="https://github.com/danielbachhuber/LEV-and-FGCH-Content-Analysis-for-January-2011-Carnival-of-Journalism/blob/master/download_and_parse_ftg_content.php">script I wrote</a> generated day URLs for the entire month (e.g. <a href="http://eastvillage.thelocal.nytimes.com/2010/11/28/">http://eastvillage.thelocal.nytimes.com/2010/11/28/</a>), downloaded the RSS feed, and parsed out the permalink and post ID for each post. <a href="http://magpierss.sourceforge.net/">MagpieRSS</a> is a simple PHP-based RSS parser for the job.

Using the permalinks identified from the RSS feed, I used another tool called <a href="http://simplehtmldom.sourceforge.net/">PHP Simple HTML DOM Parser</a> to download the entire HTML page for the post, and then parse out the data points I needed, including author, author type, body content, post category and tags. Once I had the data, I saved it to the database.

The only downside to accessing the original permalinks by RSS is that WordPress offers 10 items per feed by default, and doesn't support pagination. If there were more than 10 posts published in a day, you'd miss data. Similarly, if there are more than 10 comments, you only get the first 10.